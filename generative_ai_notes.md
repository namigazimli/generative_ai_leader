# Four primary ways to use Generative AI #

## Create ##

Using generative AI to create new content, such as:

Writing articles, emails, and social media posts.

- Generate text responses to customer service inquiries using Conversational AI.
- Draft emails to clients using Gemini in Gmail.

Creating images, videos, and audio

- Create a custom image for a presentation using Gemini in Google Slides.
- Generate multiple design concepts for brainstorming marketing materials such as website pages using Imagen.
- Create video tutorials to guide customers through troubleshooting steps using Google Vids.

Generating code in various programming languages

- Generate unit tests and other test cases for code, reducing the manual effort required for testing while still ensuring the code works as expected.
- Generate code as a starting point for larger software development projects using Gemini Code Assist.

## Summarize ##

Condense large amounts of information into concise summaries, such as:

Summarizing long documents or articles

- Summarize a lengthy financial report without having to read the entire document using NotebookLM Enterprise.
- Summarize customer reviews to understand product feedback and sentiment using Gemini in Looker.

Extracting key takeaways from meetings or presentations

- Project managers can automatically generate meeting notes that highlight key decisions and action items using Gemini in Google Meet.
- Sales teams can use summaries to quickly recap client calls and identify next steps.

Creating concise reports from complex data

- Transform complex data into easily understandable visuals like graphs using Gemini in BigQuery.

## Discover ##

Help your customers and employees find what they need at the right time, such as:

Uncovering hidden patterns and insights in data

- Analyze purchase history and website traffic to predict future demand and optimize inventory using forecasting models on Vertex AI.
- Identify viewing patterns of streaming viewers to recommend personalized content to subscribers, increasing engagement and reducing churn using Vertex AI Search.
- Analyze vast datasets of molecular structures to identify potential drug candidates with higher success rates by fine-tuning models on Vertex AI.

Searching for resources or documents

- Search for a file or ask questions about its contents using Gemini in Google Drive.
- Unlock enterprise expertise for employees with agents that bring together Gemini’s advanced reasoning, Google-quality search, and enterprise data, regardless of where it’s hosted using Google Agentspace.

Monitoring real-time events

- Use anomaly detection to identify fraudulent transactions and prevent financial losses.

## Automate ##

Automate tasks that previously required human intervention, such as:

Automated format conversation

- Convert content to accessible formats in real time by using Google Cloud's APIs, like Text-to-Speech, Speech-to-Text, Translation, and more.

Automated documentation

- Analyze code and automatically generate clear and concise documentation, saving developers time and ensuring consistency using Codey.
- Automate contract review and information extraction with Document AI.

Automated notifications and alarms

- Automate customer feedback analysis and ticket creation.

Multimodal GenAI - Gen AI can process and integrate information from diverse formats like plain text, images, videos, audio, and PDFs. This capability–known as multimodal gen AI–means that models can process various input types individually or combine multiple kinds of data simultaneously.

Vertex AI is Google Cloud's unified machine learning (ML) platform. It empowers you to build, train, and deploy ML models and AI applications. For multiple modalities (text, code, images, speech), Vertex AI gives you access to Google's large generative AI models through Model Garden. You can tune Google's LLMs to meet your needs, and then deploy them for use in your AI-powered applications.

Artificial Intelligence (AI) - Computer systems perform tasks that typically require human intelligence.
Machine Learning (ML) - Systems learn from data to solve specific problems.

Gemini - Trained on a massive dataset of text, images, code, audio, video, and more. This multimodal training allows Gemini to perform tasks across various domains, including language understanding, image generation, code generation, and more.
Imagen - Trained primarily on a massive dataset of images and text descriptions. This enables Imagen to generate high-quality images from text descriptions, perform image editing tasks, and understand the content of images.
Chirp - Trained on a large dataset of audio in various languages. Chirp is designed for speech recognition and can be used for tasks like voice assistants, transcription, and translation. Find information at the right time.

Foundation models, like other AI models, take in inputs, which are called prompts, to produce outputs.
While all AI models have inputs or prompts, foundation models are usually much more flexible in what you can prompt. For example, some AI models have strict requirements on the type of inputs they can handle. They may only handle numbers, emails, or images.
Foundation models, and especially multimodal foundation models, can take in a much broader set of inputs.

Top-down and bottom-up strategy
Executives and high-level management - Executive sponsorship and a clear vision for gen AI implementation play critical roles in gen AI implementation. Ensure that gen AI initiatives align with overall business goals, receive adequate resources, and gain organization-wide support, while simultaneously empowering employees and teams to experiment.
Mid-level managers and individual contributors (ICs) - Proximity to daily operations and end-users provides invaluable insight. Champion gen AI adoption by identifying high-impact, feasible solutions that address specific challenges within your workflows. Encourage experimentation and feedback from your teams to ensure gen AI tools are effectively integrated into existing processes, fostering increased productivity and innovation.

Successful gen AI implementation requires a combination of top-down and bottom-up approaches. For both approaches, you want to be strategizing on:

- Strategic focus
- Exploration
- Responsible AI
- Resourcing
- Impact
- Continuous improvement

Strategic focus:

1. Executives and high-level management.

- Focus on a specific area where gen AI can have a significant impact. Once you have achieved success in that initial domain, expand to other areas of your business. This focused approach maximizes learning and scalability while ensuring tangible business value.
- Prioritize use cases that are feasible to implement, actionable, affordable, and have high anticipated business value and return on investment (ROI).

2. Mid-level managers and ICs.

- Identify the most pressing pain points in your daily work or those you hear directly from customers. These real-world challenges are often the best starting point for finding gen AI applications that can have a significant impact.
- Prioritize understanding user needs, their workflows, and their expectations for interacting with AI systems. A human-centered design approach is essential for ensuring that gen AI solutions meet user needs and deliver a positive experience.
- Explore use cases in areas of the business that offer a low-risk environment for initial implementation. This allows for valuable learning and experimentation during the initial stages of adoption, enabling teams to refine processes and maximize benefits as confidence grows.

Exploration:

1. Executives and high-level management.

- Empower employees to experiment with gen AI and identify potential use cases. This can involve hosting hackathons or providing dedicated time and resources for exploration.
- Foster a collaborative environment where employees can share findings, insights, and best practices for using gen AI. This leverages the expertise and creativity of individuals across the organization.

2. Mid-level managers and ICs.

- Experiment with different gen AI tools and applications. This practical experience will provide valuable insights into the capabilities and limitations of the technology, helping to identify promising use cases.
- Share your findings and results with your team and colleagues. If initial experiments prove successful, consider expanding the scope of your exploration and bringing your insights to the attention of leadership.

Responsible AI:

1. Executives and high-level management.

- Establish ethical guidelines and implement safety mechanisms to ensure that gen AI is used responsibly and securely across the organization.
- Institute robust data governance practices that support data security and privacy, and follow relevant regulations. Use enterprise tooling like Google Cloud Data Loss Prevention, Cloud Key Management Service, and Chronicle Security Operations to automate data discovery, classification, and de-identification, manage encryption keys, and detect and respond to security threats.
- Implement a content moderation policy that includes employee review of AI-generated content, particularly in sensitive or high-risk applications.
- Regularly monitor the performance and impact of gen AI systems to identify and address any unintended consequences or ethical concerns.

2. Mid-level managers and ICs.

- Prioritize responsible AI development and deployment by adhering to company AI standards, aligning efforts with organizational values and mitigating potential risks.
- Think proactively about potential pitfalls and unintended consequences that might arise from gen AI implementation.
- Conduct thorough testing and evaluation to identify and address any biases, safety concerns, or ethical issues.

Resourcing:

1. Executives and high-level management.

- Create a robust data strategy to ensure your teams can access high-quality, relevant data that is appropriate for training and fine-tuning gen AI models.
- Avoid reinventing the wheel by leveraging existing tools and platforms where possible.
- Invest in developing the necessary AI talent within the organization by upskilling existing employees and hiring new talent with gen AI expertise.

2. Mid-level managers and ICs.

- Leverage the resources readily available within your team or department. This could include existing datasets, tools, or platforms that can be leveraged for gen AI development.
- If your ideas or experiments require additional resources, do not hesitate to speak to leadership and make a case for the potential value and ROI of your gen AI initiatives.

Impact:

1. Executives and high-level management.

- Set clear goals and define key performance indicators (KPIs) to measure the impact of gen AI, like increased sales or reduced customer complaints.
- Regularly communicate the progress and impact of gen AI initiatives to stakeholders, showcasing the value generated and justifying continued investment.

2. Mid-level managers and ICs.

- Demonstrate the tangible impact of your gen AI experiments to gain support and drive further adoption. Quantify the achievement, whether it is increased efficiency, improved user satisfaction, or cost savings.
- Articulate how your gen AI initiatives contribute to the organizations overall business goals and objectives to align your efforts with strategic priorities and showcase the value you are delivering.
- Define and track KPIs that measure the effectiveness of your gen AI solutions. This data provides evidence of your impact and helps you refine your approach over time.

Continuous improvement:

1. Executives and high-level management.

- Embrace an iterative development approach, continuously refining gen AI solutions based on feedback, performance data, and evolving business needs.
- Establish a regular evaluation cadence to assess the quality, accuracy, and effectiveness of gen AI outputs. Use this evaluation data to identify areas for improvement and guide future development efforts.
- Implement mechanisms for gathering feedback from users, stakeholders, and other relevant parties to ensure gen AI systems meet user needs and expectations.

2. Mid-level managers and ICs.

- Perform continuous testing, measurement, and refinement based on user feedback and real-world performance data. These are essential to ensure gen AI solutions remain effective and valuable.
- Update your gen AI solutions regularly based on new data and feedback to improve performance.
- Continue your own gen AI education by taking courses, reading articles, and experimenting.

Use gen AI to enhance or augment your strategic thinking for:

- Critical thinking and problem solving: Gen AI can provide data and insights, but humans are still needed to interpret those insights and make informed decisions.
- Creativity and innovation: Gen AI can assist in generating ideas and exploring possibilities, but human ingenuity is still essential for pushing boundaries and developing truly innovative solutions.
- Relationship building and collaboration: Gen AI can facilitate communication and information sharing, but strong interpersonal skills are still crucial for building trust, fostering collaboration, and navigating complex human dynamics.
- Strategic planning and vision: Gen AI can help with forecasting and trend analysis, but human leadership is essential for setting a long-term vision, defining goals, and charting a course for the future.

Use gen AI to automate tasks that are:

- Repetitive and rule-based: data entry, information retrieval, content formatting, and basic code generation.
- Time-consuming and resource-intensive: research, data analysis, content summarization, and initial draft creation.

Even for task automation, humans-in-the-loop are a necessary component for the gen AI implementation and continuous improvement. Use people for:

- Data selection and preparation: ensuring that gen AI models are trained on high-quality, relevant data that is representative of the intended use cases.
- Prompt design and refinement: crafting prompts that elicit accurate and useful responses from gen AI models.
- Output evaluation and refinement: reviewing and editing gen AI-generated content to ensure accuracy, relevance, and alignment with brand guidelines.
- Continuous monitoring and feedback: providing feedback on gen AI performance and identifying areas for improvement.

AI Augmentation implies using machine learning and natural language processing to help make better decisions. AI systems are built to work with humans, proposing suggestions or insights but leaving the final call to the individual.
AI Automation implies creating systems that perform tasks without needing any human intervention. Imagine bots handling repetitive tasks like processing invoices or AI-powered chatbots that deal with customer queries from start to finish. Here, AI is replacing human effort by doing the job itself.

Artificial intelligence (AI): The broad field of building machines that can perform tasks requiring human intelligence.
Machine learning (ML): A subset of AI where machines learn from data.
Generative AI: An application of AI that creates new content.

## Data quality ##

1. Accuracy - If the data is inaccurate, the model will learn incorrect patterns and make faulty predictions. Imagine teaching a child about animals using a book with mislabeled pictures—they would learn the wrong things. The same applies to AI.
2. Completeness - Completeness refers to the size of a dataset as well as representation within the dataset. The size of the dataset is important because the model needs enough to make an accurate prediction. If a meteorologist tries to predict weather only based on the data of the past day, it will be a much worse prediction than if it used a much larger sample size.
3. Representative - Data needs to be representative and inclusive, otherwise it can lead to skewed samples and biased outcomes. If a dataset about customer preferences is missing information about a certain demographic, the model might make inaccurate or biased generalizations about that group.
4. Consistency - Inconsistent data formats or labeling can confuse the model and hinder its ability to learn effectively. Imagine trying to assemble a puzzle where some pieces are labeled with numbers and others with letters—it would be a mess.
5. Relevance - The data must be relevant to the task the AI is designed to perform. For example, data about traffic patterns in London is unlikely to be helpful for predicting crop yields in Kansas.

## Data accessibility ##

1. Availability - If the necessary data simply is not available, the AI model can not be trained. For some problems, the data might exist, but it might be locked behind paywalls or restricted due to privacy concerns.
2. Cost - Gathering and cleaning data can be expensive. The cost of acquiring high-quality data can be a significant barrier to AI development, especially for smaller organizations.
3. Format - Data needs to be in a format that the AI model can understand and process. Converting data into the appropriate format can be time-consuming and technically challenging.

---

# Machine learning approaches and data requirements #

The quality, accessibility, and form of data are fundamental, but how that data is used for ML depends on the specific learning method. Machine learning has three primary learning approaches: supervised, unsupervised, and reinforcement learning, each with its own data requirements.
Supervised models rely on labeled data, unsupervised models work with unlabeled data, and reinforcement learning learns through interaction and feedback.

Labeled data  has tags, such as a name, type, or number. These tags, whether applied manually or by automated systems, assign meaning to the data.
For instance, an image dataset for training a cat-detection model would label each picture as either a cat or dog. Similarly, a set of customer reviews might be labeled as positive, negative, or neutral. These labels enable algorithms to learn relationships and make accurate predictions.

Unlabeled data is simply data that is not tagged or labeled in any way. It is raw, unprocessed information without inherent meaning.
Examples of unlabeled data include a collection of unorganized photos, a stream of audio recordings, or website traffic logs without user categorization. In these cases, algorithms must independently discover patterns and structures within the data, as there are no pre-existing labels to guide the learning process.

Supervised machine learning - trains models on labeled data, where each input is paired with its correct output, allowing the model to learn the relationship between them. The models goal is to identify patterns and relationships within this labeled data, enabling it to accurately predict outputs for new, unseen inputs.
Predicting housing prices is a common example of supervised learning. A model is trained on a dataset where each house has labeled data, such as its size, number of bedrooms, location, and the corresponding sale price. This labeled data allows the algorithm to learn the relationship between the features of a house and its price. Once trained, the model can then predict the price of a new house based on its features.

Unsupervised ML models - deal with raw, unlabeled data to find natural groupings. Instead of learning from labeled data, it dives headfirst into a sea of unlabeled data.
For example, an unsupervised learning algorithm could analyze customer purchase history from your companys database. It might uncover hidden segments of customers with similar buying habits, even though you never explicitly labeled those segments beforehand. This can be incredibly valuable for targeted marketing or product recommendations.
Think of it as exploratory analysis. Unsupervised learning helps you understand the underlying structure of your data and uncover insights that you might not have even known to look for.

## Reinforcement learning ##

Reinforcement learning is all about learning through interaction and feedback. Imagine a robot learning to navigate a maze. It starts with no knowledge of the maze's layout. As it explores and interacts with the maze, it collects data—bumping into walls (negative feedback) or finding shortcuts (positive feedback). Through this process of trial and error, the algorithm learns which actions lead to the best outcomes. It's like training a pet. You reward good behavior and discourage bad behavior. And over time, the pet learns to perform the desired actions. Similarly, in reinforcement learning, the algorithm learns to maximize rewards and minimize penalties by interacting with its environment. This type of learning is particularly useful in situations where you can't provide explicit instructions or labeled data. For example, you could use reinforcement learning to train a self-driving car to navigate complex traffic situations or to optimize the performance of a robot in a manufacturing plant.

## Examples of machine learning approaches on Google Cloud ##

1. Predictive maintenance with Vertex AI (supervised learning) - By training a model on sensor data from machines like temperature, pressure, and vibration, Vertex AI can predict when a machine is likely to fail, enabling proactive maintenance and reducing downtime.

2. Anomaly detection with BigQuery ML (unsupervised learning) - BigQuery ML can analyze historical transaction data (amount, location, time, etc.) to identify patterns and flag unusual transactions that deviate significantly from the norm. This helps prevent fraud and minimize financial losses.

3. Product recommendations with Vertex AI (reinforcement learning) - Vertex AI can train a reinforcement learning model to recommend products to users based on their browsing history, purchase behavior, and other factors. The model learns to maximize user engagement and sales by continuously refining its recommendations.

## Data tools and management for ML workloads ##

Google Cloud offers a comprehensive suite of data tools and management capabilities tailored for ML workloads. These tools and capabilities are designed to support the entire ML lifecycle, from data preparation and model training to deployment and monitoring.

1. Gather the data - Data gathering, also called data ingestion, involves collecting raw data from various sources. To effectively train and test your model, determine the data you need based on the outcome you want to achieve. Google Cloud supports data ingestion through several tools.

- Pub/Sub handles real-time streaming data processing, regardless of the structure of the data.
- Cloud Storage is well-suited for storing unstructured data.
- Cloud SQL and Cloud Spanner are used to manage structured data.

2. Prepare your data - Data preparation is the process of cleaning and transforming raw data into a usable format for analysis or model training. This involves formatting and labeling data properly. Google Cloud offers tools like BigQuery for data analysis and Data Catalog for data governance. These tools help prepare data for ML models.

- With BigQuery, you can filter data, correct its inconsistencies, and handle missing values.
- With Data Catalog, you can find relevant data for your ML projects. This tool provides a centralized repository to easily discover datasets in your organization.

3. Train your model - The process of creating your ML model using data is called model training. Google Cloud's Vertex AI platform provides a managed environment for training ML models.
With Vertex AI, you can set parameters and build your model, using prebuilt containers for popular machine learning frameworks, custom training jobs, and tools for model evaluation. Vertex AI also provides access to powerful computing resources to make the model training process faster.

4. Deploy and predict - Model deployment is the process of making a trained model available for use.
Vertex AI simplifies this by providing tools to put the model into action for generating predictions. This includes scaling the deployment, which means adjusting the resources allocated to the model to handle varying amounts of usage.

5. Manage your model - Model management is the process of managing and maintaining your models over time. Google Cloud offers tools for managing the entire lifecycle of ML models. This includes the following.

- Versioning: Keep track of different versions of the model.
- Performance tracking: Review the model metrics to check the model's performance.
- Drift monitoring: Watch for changes in the model's accuracy over time.
- Data management: Use Vertex AI Feature Store to manage the data features the model uses.
- Storage: Use Vertex AI Model Garden to store and organize the models in one place.
- Automate: Use Vertex AI Pipelines to automate your machine learning tasks.

All these Google Cloud tools are designed to work together seamlessly. You can also connect them with your existing tools or build your own custom solutions. And throughout the entire process, Google Cloud's infrastructure ensures your solution is available and reliable. At the same time, Identity and Access Management (IAM) keeps your data safe and ensures only the right people have access.

---

# Deep learning #

A broad field that encompasses many different techniques, one of which is deep learning (DL). Deep learning is a powerful subset of machine learning, distinguished by its use of artificial neural networks. These networks enable the processing of highly complex patterns and the generation of sophisticated predictions. Neural networks can leverage both labeled and unlabeled data, a strategy known as semi-supervised learning. They train on a blend of a small amount of labeled data and a large amount of unlabeled data. That way, they learn foundational concepts and generalize effectively to novel examples. Generative AI uses the power of deep learning to create new content spanning text, images, audio, and beyond. Deep learning techniques, particularly those centered on neural networks, are the engine behind these generative models.

Remember foundation models from course one? Foundation models use deep learning. They are trained on massive datasets that allow them to learn complex patterns and perform a variety of tasks across different domains. They are incredibly powerful machine learning models trained on a massive scale, often using vast amounts of unlabeled data. This training allows them to develop a broad understanding of the world, capturing intricate patterns and relationships within the data they consume. Think of a foundation model as a highly advanced learner. It's like a student who has read everything in an entire library, absorbing knowledge from countless books, articles, and websites. This deep and extensive learning allows foundation models to be adapted to a wide range of tasks.

Large Language Models (LLMs) - One particularly exciting type of foundation model is the LLM. These models are specifically designed to understand and generate human language. They can translate languages, write different kinds of creative content, and answer your questions in an informative way, even if they are open ended, challenging, or strange. This is likely the most common foundation model you've encountered, such as in popular generative AI chatbots like Gemini. They also help power many search engines you use today.

Diffusion models - Diffusion models are another type of foundational model. They excel in generating high-quality images, audio, and even video by iteratively refining noise (or unstructured/random data and patterns) into structured data.

## Factors when choosing a model for your use case ##

1. *Modality* - When selecting a generative AI model, it's crucial to consider the modality of your input and output. Modality refers to the type of data the model can process and generate, such as text, images, video, or audio. If your application focuses on a single data type, like generating text-based articles or creating audio files, you'll want to choose a model optimized for that specific modality. For applications that require handling multiple data types, such as generating image captions (processing images and producing text) or creating video with accompanying audio, you'll need a multimodal model. These models can understand and synthesize information across different modalities.

2. *Context window* - The context window refers to the amount of information a model can consider at one time when generating a response. A larger context window allows the model to "remember" more of the conversation or document, leading to more coherent and relevant outputs, especially for longer texts or complex tasks. However, larger context windows often come with increased computational costs. You need to balance the need for context with the practical limitations of your resources.

3. *Security* - Security is paramount, especially when dealing with sensitive data. Consider the model's security features, including data encryption, access controls, and vulnerability management. Ensure the model complies with relevant security standards and regulations for your industry.

4. *Availability and reliability* - The availability and reliability of the model are crucial for production applications. Choose a model that is consistently available and performs reliably under load. Consider factors like uptime guarantees, redundancy, and disaster recovery mechanisms.

5. *Cost* - Generative AI models can vary significantly in cost. Consider the pricing model, which might be based on usage, compute time, or other factors. Evaluate the cost-effectiveness of the model in relation to your budget and the expected value of your application. This is where selecting the right model for the right task is important. Be sure to match the model to the task; bigger isn't always better, and multi-modal capabilities aren't always necessary.

6. *Performance* - The performance of the model, including its accuracy, speed, and efficiency, is a critical factor. Evaluate the model's performance on relevant benchmarks and datasets. Consider the trade-offs between performance and cost.

7. *Fine-tuning and customization* - Some models can be fine-tuned or customized for specific tasks or domains. If you have a specialized use case, consider models that offer fine-tuning capabilities. This often involves training the model further on a specific dataset related to your use case.

8. *Ease of integration* - The ease of integrating the model into your existing systems and workflows is an important consideration. Look for models that offer well-documented APIs and SDKs.

---

# Google Cloud's ML models #

**Vertex AI** streamlines the integration of advanced artificial intelligence capabilities into business applications, allowing for seamless discovery, deployment, and customization. These models empower businesses to leverage cutting-edge AI, providing the flexibility to work with many different models without the need for extensive in-house model development.
With Vertex AI you can access models developed by Google including Gemini, Gemma, Imagen, and Veo. You can also access proprietary third-party models, and openly available models.

**Gemini** - Gemini, a multimodal model, can understand and operate across diverse data formats, such as text, images, audio, and video. Gemini's multimodal design supports applications that require complex multimodal understanding, advanced conversational AI, content creation, and nuanced question answering.

**Gemma** - A family of lightweight, open models is built upon the research and technology behind Gemini. They offer developers a user-friendly and customizable solution for local deployments and specialized AI applications.

**Imagen** - A powerful text-to-image diffusion model, it excels at generating high-quality images from textual descriptions. This makes it invaluable for creative design, ecommerce visualization, and content creation.

**Veo** - A model capable of generating video content. It can produce videos based on textual descriptions or still images. Its functionality allows for the creation of moving images for applications such as film production, advertising, and online content.

Collectively, these Google Cloud foundation models empower businesses to enhance customer experiences through intelligent chatbots and personalized content. They increase productivity by automating tasks and improving information retrieval. They foster innovation by generating new ideas and designs. They also derive data-driven insights for improved decision-making.

---

# Google strategies for foundation model limitations #

## Foundation model limitations ##

1. **Data dependency** - The performance of foundation models is heavily data-dependent. They require large datasets, and any biases or incompleteness in that data will inevitably seep into their outputs. It's like asking a student to write an essay on a book they haven't read. If the data or questions are inaccurate or biased, the AI's performance will suffer.

2. **Knowledge cutoff** - Knowledge cutoff is the last date that an AI model was trained on new information. Models with older knowledge cutoffs may not know about recent events or discoveries. This can lead to incorrect or outdated answers, since AI models don't automatically update with the latest happenings around the world. For example, if an AI tool's last training date was in 2022, it wouldn't be able to provide information about events or information that happened after 2022.

3. **Bias** - An LLM learns from large amounts of data, which may contain biases. You can think of bias as an unbalanced dataset in LLMs. Due to their statistical learning nature, they can sometimes amplify existing biases present in the data. Even subtle biases in the training data can be magnified in the model's outputs.

4. **Fairness** - Even with perfectly balanced data, defining what constitutes fairness in an LLM's output is a complex task. Fairness can be interpreted in various ways. Fairness assessments for generative AI models, while valuable, have inherent limitations. These evaluations typically focus on specific categories of bias, potentially overlooking other forms of prejudice. Consequently, these benchmarks do not provide a complete picture of all potential risks associated with the models' outputs, highlighting the ongoing challenge of achieving truly equitable AI.

5. **Hallucinations** - Foundation models can sometimes experience hallucinations, which means they produce outputs that aren't accurate or based on real information. Because foundation models can't verify information against external sources, they may generate factually incorrect or nonsensical responses. These cause significant concern in accuracy-critical applications. The responses might sound convincing, but they are completely wrong.

6. **Edge cases** - Rare and atypical scenarios can expose a model's weaknesses, leading to errors, misinterpretations, and unexpected results.

## Techniques to overcome limitations ##

1. **Grounding** - Generative AI models are amazing at creating content, but sometimes they hallucinate. Grounding is the process of connecting the AI's output to verifiable sources of information—like giving AI a reality check. By providing the model with access to specific data sources, we tether its output to real-world information, reducing the risk of invented content. Grounding is essential for building trustworthy and reliable AI applications. By connecting your models to verifiable data, you ensure accuracy and build confidence. It offers several key benefits, including reducing hallucinations, which prevents the AI from generating false or fictional information. Grounding also anchors responses, ensuring the AI's answers are rooted in your provided data sources. Furthermore, it builds trust by enhancing the trustworthiness of the AI's output by providing citations and confidence scores, allowing you to verify the information.

2. **Retrieval-augmented generation (RAG)** - There are many different options on how you can ground in data. For example, you can ground in enterprise data or you can ground using Google Search. One common grounding method to do this is with **retrieval-augmented generation**, or **RAG**. RAG is a grounding method that uses search to find relevant information from a knowledge base and provides that information to the LLM, giving it necessary context. The first step is retrieval. When you ask an AI a question, RAG uses a search engine to find relevant information. This search engine uses an index that understands the semantic meaning of the text, not just keywords. This means it finds information based on meaning, ensuring higher relevance. The retrieved information is then added to the prompt given to the AI. This is the augmentation phase. The AI then uses this augmented prompt, along with its existing knowledge, to generate a response. This is referred to as the generation phase.

3. **Prompt engineering** - Prompting offers the most rapid and straightforward approach to supplying supplementary background information to models. This involves crafting precise prompts to guide the model towards desired outputs. It refines results by understanding the factors that influence a model's responses. However, prompting is limited by the model's existing knowledge; it can't conjure information it hasn't learned.

4. **Fine-tuning** - When prompt engineering doesn't deliver the desired outcomes, fine-tuning can enhance your model's performance. Pre-trained models are powerful, but they're designed for general purposes. Tuning helps them excel in specific areas. This process is particularly useful for specific tasks or when you need to enforce specific output formats, especially if you have examples of the desired output. Tuning involves further training a pre-trained or foundation model on a new dataset specific to your task. This process adjusts the model's parameters, making it more specialized for your needs. Google Cloud Vertex AI provides tooling to facilitate tuning. Here are some examples of how tuning can be used:

- Fine-tuning a language model to generate creative content in a specific style.
- Fine-tuning a code generation model to generate code in a particular programming language.
- Fine-tuning a translation model to translate between specific languages or domains.

## Humans in the loop (HITL) ##

Beyond these techniques, we must remember the invaluable role of humans in the loop (HITL). Machine learning models are powerful, but they sometimes need a human touch. For tasks requiring judgment, context, or handling incomplete data, human expertise is essential. HITL systems integrate human input and feedback directly into the ML process. This collaboration makes models more adaptable, especially in areas like the following.

***Content moderation*** - HITL ensures accurate and contextually appropriate moderation of user-generated content, filtering out harmful or inappropriate material that algorithms alone might miss.
***Sensitive applications*** - In fields like healthcare or finance, HITL provides oversight for critical decisions, ensuring accuracy and mitigating risks associated with automated systems.
***High-risk decision making*** - When ML models inform decisions with significant consequences, such as medical diagnoses or criminal justice assessments, HITL acts as a safeguard, providing a layer of human review and accountability.
***Pre-generation review*** - Before deploying ML-generated content or decisions, human experts can review and validate the outputs, catching potential errors or biases before they impact users.
***Post-generation review*** - After ML outputs are deployed, continuous human review and feedback help identify areas for improvement, enabling models to adapt to evolving contexts and user needs.

---

# Secure AI #

Secure AI is about preventing intentional harm being done to your applications. This is about protecting AI systems from malicious attacks and misuse. For all applications, including AI, you need to ensure security throughout the full lifecycle from development through deployment. This includes considering the data, infrastructure, and how and where applications are deployed. To better understand how these security principles apply, let's reexamine the machine learning lifecycle from an earlier lesson.

**Gather your data** - The foundation of any robust AI system is secure data. Therefore, data must be protected and secured at all times. Access controls must be implemented to restrict who can access the data. They are also necessary to restrict who can add or input to the data. This is crucial to prevent data poisoning, a malicious attack where bad actors corrupt your AI model’s training data by introducing manipulated data. This can cause your model to learn incorrect patterns and make flawed (biased, inaccurate, or even harmful) predictions. It's akin to someone secretly swapping out vital ingredients in a recipe, leading to a disastrous final dish.

**Prepare your data** - During data preparation, special attention should be paid to confidential or sensitive data that might be present in the training data. Where feasible, anonymization techniques should be applied to this data, mitigating potential privacy risks. Data also should be rigorously validated, using integrity checks and anomaly detection to prevent poisoning. Data should be securely processed with encryption both at rest and in use. Logging and real-time monitoring should also be performed.

**Train your model** - Securing the model and the training process is paramount. This means safeguarding both the training data and model parameters from unauthorized access or modification. Model theft, where attackers steal proprietary or sensitive AI models, is a significant threat. Beyond gaining a competitive advantage, the stolen models could be used for malicious purposes, exploiting vulnerabilities or replicating sensitive functionality.

**Manage your model** - Continuous monitoring and maintenance of the model's security are essential. Stay up to date on the latest security best practices in the field, specifically for your platform and model, and regularly update to patch vulnerabilities. Monitor model performance and outputs for anomalies or signs of tampering, and regularly review access permissions.

**Deploy and predict** - Once your model is trained and ready for action, it's crucial to safeguard it within its operating environment. This means controlling access to the model, determining who can interact with it and how. If you're using a pre-built model, it's essential to verify its source and check for any potential vulnerabilities.

Think of it like installing a high-tech security system in your office. You wouldn't give everyone the master key, right? Similarly, you need to restrict access to your model and filter the information it receives and sends out. One major risk here is adversarial attacks, where attackers try to trick the model with misleading information. Imagine someone trying to bypass your security system with a fake ID—that's essentially what an adversarial attack does to your AI. Therefore, it's critical to have safeguards in place to filter, sanitize, and check the data coming into your model, ensuring it's not being fed misleading information. Likewise, you need to monitor the information your model sends out. Think of it as screening outgoing messages to prevent any accidental leaks of sensitive data or the spread of harmful content.

Keeping AI applications secure demands constant vigilance and staying informed about the latest security threats and research.

**Applying the Secure AI Framework (SAIF)** - Google has developed the Secure AI Framework, or SAIF, to establish security standards for building and deploying AI systems responsibly. This comprehensive approach to AI/ML model risk management addresses the key concerns of security professionals in the rapidly evolving landscape of AI. Following the security practices outlined in SAIF can help your organization find and stop threats, automatically strengthen its defenses, and manage the unique risks of each AI system. The framework is designed to integrate with your company’s existing security, ensuring that AI models are secure by default. For more information, explore [Google's Secure AI Framework (SAIF)](https://safety.google/cybersecurity-advancements/saif/).

**Google Cloud security tools** - Platforms such as Google Cloud can facilitate secure development. In addition to the Secure AI Framework, Google Cloud offers a range of tools to ensure applications remain secure throughout their lifecycle. Google Cloud has built security into its core through a secure-by-design infrastructure, encompassing its global network and hardware, as well as robust encryption in transit and at rest. It provides customers with detailed control over access and usage of cloud resources through Identity and Access Management (IAM). Google Cloud Security Command Center provides a centralized view of your security posture across your entire Google Cloud environment, and Google Cloud also offers tools for monitoring various workloads.

---

# Responsible AI #

Responsible AI means ensuring your AI applications avoid intentional and unintentional harm. It’s important to consider ethical development and positive outcomes for the software you develop. The same holds true, and perhaps even more so, for AI applications. The foundation of responsible AI is security. Secure applications protect both your company and your users. Think of it like building a house: if the foundation is weak, the entire structure is compromised, no matter how beautiful the design. Just as a strong foundation ensures a stable and safe house, robust security forms the essential foundation for building truly responsible AI.

**Transparency is key** - Transparency is paramount for responsible applications. Users need to understand how their information is being used and how the AI system works. This transparency should extend to all aspects of the AI's operation, including data handling, decision-making processes, and potential biases.

**Privacy in the age of AI** - Protecting privacy often involves anonymizing or pseudonymizing data, ensuring individuals can't be easily identified. AI models can sometimes inadvertently leak sensitive information from their training data, so it's crucial to implement safeguards to prevent this.

**Data quality, bias, and fairness**

1. Ethical AI requires high quality data - Machine learning and generative AI applications are fundamentally based on data. Therefore, responsible AI requires high-quality data. Inaccurate or incomplete data can lead to biased outcomes. Remember, technology often reflects existing societal biases. Without careful consideration, AI can amplify these biases, leading to unfair and discriminatory results. Beyond data quality, it's crucial to consider the responsible use of the data itself. Was it collected consensually? Could it perpetuate harmful biases?

2. Understanding and mitigating bias - AI systems are not independent of the world they are built in. They can inherit and amplify existing societal biases. This can lead to unfair outcomes, such as a resume-screening tool that favors a certain demographic of candidates due to historical biases in hiring data. It's like training a dog with biased commands; the dog will learn and replicate those biases. To counter this, fairness must be a core principle in AI development.

**Accountability and explainability**

1. Fairness requires accountability - Fairness requires accountability. We need to know who is responsible for the AI's actions and understand how it makes decisions. This is where explainability comes in. Explainable AI makes the decision-making processes of AI models transparent and understandable. This is crucial for building trust, debugging errors, and uncovering hidden biases. Think of it like a judge explaining their verdict; without a clear explanation, it's hard to trust the decision. Tools like Google Cloud’s Vertex Explainable AI can help understand model outputs and identify potential biases. Understanding how your application uses and interprets the AI's output is crucial for ensuring responsible use.

---

# Understanding the layers of gen AI #

Think of generative AI as a powerful engine. The infrastructure is its foundation, the models are its core, and the platform connects everything together. Agents act as the drivers, making decisions and taking action, while applications are the vehicles that take us to our destination.

1. Infrastructure - The foundation of any AI system, comprising the hardware (physical servers, cloud computing resources, specialized chips like GPUs and TPUs) and software (operating systems, networking) that provide the necessary computing power, storage, and connectivity to train, deploy, and scale AI models.

2. Models - The model is where the brains of the AI system reside. It comprises various algorithms that learn patterns from data and can make predictions or generate new content. Examples are large language models (LLMs) like Gemini, image recognition models, and recommendation systems.

3. Platform - This is the layer that consists of tools and services that help with building and deploying AI models. This includes model training platforms like Vertex AI and data management tools.

4. Agent - This layer utilizes the capabilities of the model layer to perform more complex actions. This layer interacts with the environment, gathers information and makes decisions and executes actions based on the information received.

5. Gen AI powered application - This is the layer that delivers the AI capability to users through interfaces.

---

# Agents and gen AI powered applications #

A gen AI agent is an application that tries to achieve a goal by observing the world and acting upon it using the tools it has at its disposal. Gen AI agents can process information, reason over complex concepts, and take actions. They can also be used in a variety of applications, including customer service, employee productivity, and creative tasks. With generative AI, you have the flexibility to leverage either pre-built agents and applications, or create custom ones tailored to your specific needs. No matter which path you choose, you can apply generative AI's capabilities in a way that best aligns with your objectives. Google Cloud tools like AI Applications help teams create AI agents using natural language rather than having to write complex code to do the same thing.

Think of agents as the intelligent pieces within a larger gen AI powered application. They bring specific capabilities to the table.

1. **Understanding and responding to natural language** - This allows applications to have more intuitive interfaces, like chatbots that can actually understand complex requests.
2. **Automating complex tasks** - Agents can handle multi-step processes within an application, such as gathering information, making decisions based on that information, and taking action.
3. **Personalization** - Agents can learn user preferences and tailor the application experience accordingly.

Think of gen AI powered applications as the user-facing layer. They provide the structure and context for agents to operate within. They define the user interface, the overall goals, and the specific tasks that agents will help with. Often, multiple agents with different specializations work together within a single application to create a richer, more dynamic user experience. This is known as a multi-agent system. You can review some examples below.

- **A travel booking app**: An agent could handle the complex task of finding the best flights and hotels based on user preferences, while another agent might specialize in suggesting relevant activities and attractions at the destination. Then, the application provides the user with the interface for browsing options and making reservations.
- **A customer support app**: An agent could answer common questions, troubleshoot problems, and escalate complex issues to human representatives. The application would provide the chat interface and integrate with other support systems.
- **A personalized learning app**: An agent could assess a student's knowledge, recommend relevant learning materials, and even generate personalized exercises. The application would provide the structure for lessons and track progress.

---

# Conversational and workflow agents #

**Conversational agents**

Conversational agents are designed to understand what you mean, not just what you say, and respond in a way that makes sense.

- ***You provide input***: You can type a message or speak to the agent.
- ***The agent understands***: Using powerful AI, it figures out the meaning and intention behind your words.
- ***The agent calls a tool***: Based on your request, the agent might need to gather additional information or perform an action. This could involve searching the web, accessing a database, or interacting with another software application.
- ***The agent generates a response***: It formulates an answer that's relevant to your request and sounds natural.
- ***The agent delivers the response***: You'll see or hear the answer depending on how you interacted with it.

Conversational agent examples.

- ***Answering questions***: Imagine you're curious about the weather in Paris. You ask a conversational agent, "What's the weather like in Paris?" And it instantly provides you with the current temperature, forecast, and even suggests what to pack if you're traveling there.
- ***Casual conversation***: Feeling bored? A conversational agent can engage in lighthearted banter, tell you jokes, or even discuss your favorite books and movies, just like a friend would.
- ***Accessing information***: Need to know the capital of Australia? Or maybe you want a quick summary of the French Revolution? A conversational agent can access a vast amount of world knowledge and provide you with concise and accurate information.

**Workflow agents**

Workflow agents are designed to streamline your work and make sure things get done efficiently and correctly by automating tasks or going through complex processes.

- **You provide input**: You define a task or trigger a process like submitting a form, uploading a file, initiating a scheduled event, or even ordering a product online.
- **The agent understands**: The agent is the software that automates those steps. It interprets the task's requirements and defines the series of steps needed to complete the task.
- **The agent calls a tool**: Based on the workflow's definition, the agent executes a series of actions. This could involve data transformation, file transfer, sending notifications, integrating with external systems, or initiating other automated processes using APIs.
- **The agent generates a result/output**: It compiles the outcome of the executed actions, which might be a report, a data file, a confirmation message, or an updated status within a system.
- **The agent delivers the result/output**: The agent delivers the output to the designated recipient(s) or system(s), such as via email, a dashboard, a database update, or a file storage location.

Workflow agents examples.

- ***Ecommerce order fulfillment***: An agent automatically processes orders, updates inventory, sends shipping notifications, and handles returns.
- ***Customer onboarding***: An agent guides new customers through account setup, provides tutorials, and answers frequently asked questions.
- ***Automated research***: An agent can conduct in-depth research on a given topic by autonomously browsing the web, summarizing relevant content, and generating comprehensive reports. (Try this out with Gemini Deep Research.)
- ***Security Log Parsing***: An agent that inspects incoming security logs for abnormalities and can flag them, open a ticket, begin triage, and assign to humans for review when necessary.

---

# Platform layer #

## Key features and benefits of Vertex AI ##

The platform layer provides the foundation for building and scaling your AI initiatives. Vertex AI is Google Cloud's unified machine learning (ML) platform designed to streamline the entire ML workflow. It provides the infrastructure, tools, and pre-trained models you need to build, deploy, and manage your ML and generative AI solutions.

1. Open and flexible - Vertex AI supports open frameworks, allowing you to use your preferred tools and models without vendor lock-in.
2. Powerful infrastructure - Leverage Google Cloud's scalable and reliable infrastructure to handle your ML workloads.
3. Pre-trained models - Choose from a wide variety of pre-trained models or build your own.
4. Comprehensive tooling - Develop, deploy, and manage your models with ease using Vertex AI's integrated tools.
5. Customization - Fine-tune and adapt models to your specific needs using the built-in IDE.
6. Easy integration - Integrate your models into applications seamlessly using available APIs.

## MLOps for efficient workflows ##

Operationalizing model training can be challenging and involves thinking about infrastructure and security. By using Vertex AI, you get a fully managed compute infrastructure, high-performance ML optimized training jobs, distributed training, hyperparameter optimization, and enterprise security. Vertex AI also includes ML operations (MLOps) tools built in to help you orchestrate end-to-end ML workflows, perform feature engineering, run experiments, manage and iterate your models, track ML metadata, and monitor and evaluate model quality. MLOps helps automate, standardize, and manage the ML project lifecycle, enabling better collaboration and continuous improvement.

- **Featura store**: Share, serve, and reuse ML features to maintain consistency and efficiency.
- **Model registry**: Manage model versions, track changes, and organize your models throughout their lifecycle.
- **Model evaluation**: Quickly evaluate and compare model performance to identify the best model for your use case.
- **Workflow orchestration**: Automate ML workflows, from data preprocessing to deployment, using Vertex AI Pipelines.
- **Model monitoring**: Monitor models for performance degradation, detect input skew and drift, and trigger updates or retraining.

---

# Model layer #

At the heart of every AI and machine learning system lies the model. Think of it as the brain of the operation. These aren't just any algorithms. They're sophisticated mathematical structures trained on massive amounts of data. This training process allows them to learn patterns and relationships, ultimately enabling them to perform a wide range of tasks, such as generating content, analyzing data, and classifying information.
Within the Vertex AI platform, you have multiple options for how to handle AI models for your project. You can choose to build a model from scratch or use an existing model.

**Use models with Model Garden** - Within the Vertex AI platform, you have multiple options for how to handle AI models for your project. You can choose to build a model from scratch or use an existing model. Model Garden gives you options to pick from over 160 models and offers options of Google models, third-party models, and open-source models. With Vertex AI, you can customize these models with your own data and deploy them to your custom applications. Additionally, some pre-trained models can be used out-of-the-box without tuning.

1. **First-party foundation models** - Leverage state-of-the-art models from Google including Gemini models and task-specific models for image generation, speech-to-text, and more.
- **Gemini Pro**, Flash, and more.
- **Imagen** for text-to-image.
- **Veo** for text-to-video and image-to-video.
- **Chirp** 2.0 for speech-to-text.
2. **First-party pre-trained APIs** - Build and deploy AI applications faster with our pre-trained APIs powered by the best Google AI research and technology.
3. **Open models** - Access a variety of enterprise-ready open models.
4. **Third-party models** - Model Garden supports third-party models from partners with foundation models.

## Build models with Vertex AI ##

With Vertex AI, you have two options for training and using your own models. You can go fully custom and create and train models at scale using any ML framework (PyTorch, TensorFlow, scikit-learn, or XGBoost), or you can use AutoML to create and train models with minimal technical knowledge and effort. AutoML allows you to build these types of models:

| Data type     | Supported objectives                                  |
| :---          | :---                                                  |
| Image data    | Classification, object detection.                     |
| Video data    | Action recognition, classification, object tracking.  |
| Text data     | Classification, entity extraction, sentiment analysis.|
| Tabular data  | Classification/regression, forecasting.               |

Follow this standard workflow when creating your models in Vertex AI:
1. **Gather your data** - Determine the data you need for training and testing your model based on the outcome you want to achieve.
2. **Prepare your data** - Make sure your data is properly formatted and labeled.
3. **Train** - Set parameters and build your model.
4. **Manage your model** - Review model metrics to evaluate the model’s performance and monitor changes over time.
5. **Deploy and predict** - Make your model available to use.

---

# Infrastructure layer

The "infrastructure layer" is the foundation upon which any AI system is built. It's the combination of hardware and software that provides the necessary computing power, storage, and networking capabilities to train, deploy, and scale AI models. This layer is crucial for handling the massive datasets and complex computations involved in modern AI, especially with the rise of large language models and generative AI. Google has invested heavily in building a robust and cutting-edge AI infrastructure, designed to handle the most demanding AI workloads. Here are some key components.

## High performance computing

Imagine your company wants to undertake a new, bold gen AI project. However, the engineering team hits a major roadblock. Training the complex model requires significant processing power, far beyond the capabilities of your company's in-house servers. It takes hours, if not days, to train your specialized models and the long training times have started to hinder the team's progress to the point where it's delaying the product launch. Your existing infrastructure simply can't keep up, and the cost of scaling their own hardware is prohibitive. This is where high-performance computing, the heart of Google's AI infrastructure, comes in.

**GPUs and TPUs** - These specialized processors are the workhorses of AI. They excel at parallel processing, or doing multiple things at once, which is crucial for training large neural networks quickly and efficiently. GPUs were originally designed for graphics rendering, but since they offer general-purpose parallel processing power they’ve increasingly been used in the AI domain. TPUs are Google's custom-designed chips specifically optimized for AI tasks.

**Hypercomputers** - A hypercomputer is essentially a supercomputer built by connecting many individual computers (nodes) together. These nodes contain GPUs and TPUs. By linking them with high-speed networks, they work together as one massive computing unit. They provide the massive scale necessary for training and running the most demanding generative AI models.

## High performance storage

Having high-performance storage is essential when building gen AI apps. Gen AI models are data hungry because they learn by analyzing massive datasets, often petabytes in size, containing text, images, code, and other forms of data. This storage provides the capacity and speed to store and access these enormous datasets efficiently. Without it, training would be incredibly slow or even impossible.

**Large-scale storage systems** - Google Cloud's storage infrastructure is optimized for AI workloads, offering high throughput (the amount of work done within a specific period of time), scalability, and the ability to create dense compute clusters for faster training and inference. As AI models and datasets grow, the system can scale seamlessly to accommodate the increasing demands. Losing data due to storage failures can be catastrophic, so the storage is highly reliable to ensure data durability.

**Fast storage access** - The storage systems also need to provide fast read and write speeds to keep up with the demands of the training process. Otherwise, if storage access is slow, it can significantly hinder the training process.

## Networking

Fast and efficient communication is essential for coordinating the work all of the processors in a high performance computing cluster are undertaking. Google's global fiber network provides high-bandwidth, low-latency connectivity, ensuring smooth data flow and efficient communication between different parts of the AI infrastructure. Using Google Cloud infrastructure can benefit teams in several ways, particularly with respect to scalability, performance, and cost-effectiveness.


---

# Edge computing

Edge computing runs AI on devices or servers closer to the data source or point of need. While hosting infrastructure on the cloud is powerful, it's not always the ideal solution. Imagine a self-driving car needing to make split-second decisions. It can't wait for data to travel to the cloud and back. That's where edge computing comes in. Google provides the tools to deploy your AI models in these different locations, giving you more control and flexibility. 
Imagine a drone navigating a complex environment. It needs to react instantly to obstacles, making cloud processing too slow. Running AI locally on the drone ensures real-time responsiveness. Other benefits include increased data privacy and reduced reliance on internet connectivity. To run powerful AI models on edge devices and mobile phones, Google provides tools like **Lite Runtime (LiteRT)**. Think of LiteRT as a platform that helps machine learning models work efficiently on your device. One example of an AI model designed for edge is Gemini Nano.

## Gemini Nano

**Gemini Nano** is Google's most efficient and compact AI model, specifically designed to run on the edge on devices like smartphones and embedded systems. It's part of the larger Gemini family of models. Think of Gemini Nano as a miniature version of the powerful AI that usually lives in Google's data centers. This "on-device" or edge approach offers several benefits. 

- **Privacy**: Your data stays on your device, enhancing your privacy.
- **Speed**: You get fast responses since there's no need to send data to the cloud.
- **Offline access**: Gemini Nano can work even without an internet connection.

Gemini Nano brings the power of AI directly to your devices, making them smarter, more helpful, and more privacy-focused. It is currently being integrated into various Google products and services, including:
- Pixel phones: Gemini Nano powers features like Call Notes, which summarizes phone conversations and Pixel Recorder, which summarizes voice recordings.
- Android: Gemini Nano is available to Android developers through the AI Edge SDK, enabling them to build innovative AI experiences into their apps.

Even when your goal is to deploy on the edge, Vertex AI provides a powerful platform for building, training, and refining your models. You can leverage Vertex AI's capabilities for tasks like data preparation, model training, tuning, model evaluation, collaboration, and MLOps. Once your model is ready for the edge, Vertex AI offers tools to streamline the deployment process. You can:
- **Convert models**: Convert your models to Lite Runtime (LiteRT) for optimal performance on edge devices.
- **Package and deploy**: Package your models and dependencies into containers for deployment on various edge hardware.
- **Manage and monitor**: Manage your edge deployments, track their performance, and gather insights to improve your models over time.
Google provides the tools to deploy your AI models in these different locations, giving you more control and flexibility.


---

# Gen AI project resources: People, cost, and time

## Roles and responsibilities

It is important to understand the resources you have available to build out your solution. Let’s dig deeper into some of the roles and responsibilities at each layer of the stack. The AI stack is designed to support different roles with specific needs. Before jumping into building out a solution, ensure you have the proper expertise and talent for what you want to build. Business users can improve their tasks, developers can build custom AI solutions, and AI practitioners can develop and deploy advanced AI models responsibly and securely. This collaborative ecosystem allows your organization to leverage the full potential of AI across various business functions.

- **Business leaders**: Business leaders typically interact with pre-built gen AI solutions to enhance daily operations and improve customer experiences. For example, Gemini for Google Workspace can be used for content creation, data analysis, and document summarization.
- **Developers**: Developers in your organization are responsible for building and deploying custom AI agents and integrating AI capabilities into existing applications. They can use AI Applications for custom agent creation, AI code generation, and AI-driven data processing. Developers can also leverage pre-trained APIs to rapidly integrate AI into applications, or they can use the Vertex AI platform which is designed to help developers build and deploy AI agents with tools for orchestration, grounding, and action.
- **AI practitioners**: AI practitioners play a valuable role in customizing, deploying, and optimizing generative AI models. They leverage tools within Vertex AI to accelerate development and ensure responsible AI practices. Their expertise extends to scaling AI workloads, integrating models with BigQuery, and implementing responsible AI measures such as bias detection and adversarial testing.

## Cost

The pricing model of gen AI can vary based on which part of the gen AI landscape you are looking at or even which specific product or company you are using. It is important to have a realistic budget and understanding of the cost for your project. When building gen AI solutions, you pay for three primary activities:
- **Training** the model.
- **Deploying** the model to an endpoint.
- **Using** the model to make predictions.
Training and deploying models often involve paying for the compute time used as well as the storage for the training data and model outputs.

### Pricing for using models

- **Usage-based**: You pay for the amount you use, often measured in tokens or characters processed. This is common for APIs like Google’s PaLM & Gemini APIs.
- **Subscription-based**: You pay a recurring fee for access to the model, often with tiers based on usage limits or features.
- **Licensing fees**: One-time or recurring fees for using a model, especially for commercial purposes or embedding in products.
- **Free tiers**: Some providers offer free access with limited usage for experimentation or non-commercial purposes.

### Pricing metrics for using models

- **Tokens**: A token represents a piece of text, like a word or part of a word.
- **Characters**: Some providers, like Google, charge based on the number of characters processed.
- **Requests**: Sometimes you can be charged per request, regardless of the complexity or volume of the task.
- **Compute time**: The time taken to process your requests can also factor into the cost, especially for resource-intensive tasks.

### Factors affecting cost

- **Model size and complexity**: Larger, more capable models generally cost more.
- **Context window**: A larger context window (the amount of text the model can consider) can increase costs.
- **Features**: Specialized features like fine-tuning or embedding can have separate pricing.
- **Deployment**: Depending on where you deploy your model and application, you may have compute based costs.

## Time

It probably won’t surprise you to learn that the more custom your solution is, the more time and resources it takes to build. Using a prebuilt gen AI application takes seconds. Building your own custom solution with a custom AI model, can take months. Choosing to build a custom agent with AI Applications takes much less time. Think about your project timelines and evaluate them against your needs and requirements. Determine if the timeline is realistic based on your requirements.


---

# Gen AI solution needs

Aligning your ambitions with your capabilities is key to setting realistic expectations and ensuring a successful outcome. Let's explore the key factors to consider when evaluating the scope and feasibility of your generative AI initiatives.

## Scale

Is this for individual use, a small team, a large company, or millions of customers?
- **Small scale**: As an individual user or small team, you can go very far with pre-built tools. Try to leverage existing gen AI powered applications.
- **Large scale**: When building out solutions for millions of customers, you'll likely need a more robust and customized solution. Pick solutions that offer scalability and security. Factor in details to your decision like infrastructure costs, data storage, and potential latency challenges.

## Customization

How specialized does your solution need to be? Powerful foundation models are constantly improving, making custom model development less necessary in many cases.

1. **Start with existing models** - Explore pre-trained models available through APIs or open-source libraries. Fine-tune these models on your specific data to achieve better performance for your tasks.
2. **Identify your unique needs** - What sets your project apart? Does it require specialized knowledge, handle complex tasks, or demand a unique user experience? This will help you determine the level of customization needed.
3. **Consider data specifity** - If your project deals with specialized domains like law or medicine, consider fine-tuning with domain-specific datasets or exploring models specifically trained for those areas.
4. **Consider task complexity** - Think about the complexity of the task itself. Are you aiming for simple tasks like text summarization or more intricate ones like code generation or creative writing? This influences model choice and training approaches.

## User interaction

Think about how users will interact with your AI and design an intuitive and engaging experience.

- **User interface (UI)**: Seamlessly integrate the AI into your existing workflows. This might involve a dedicated interface (e.g., a chatbot widget on your website) or embedding AI capabilities within your current applications.
- **User experience(UX)**: Aim for a user-friendly experience. Is it conversational, informative, or task-oriented? Consider the level of guidance and feedback users might need.

## Privacy

How sensitive is the data you are working with?

- **Data security**: What measures will you implement to protect data during processing and storage? Implement robust security measures to protect data during processing and storage. Consider encryption, access controls, and secure data centers.
- **Compliance**: Are there specific regulations (GDPR, HIPAA, etc.) that your project needs to adhere to?

## Other considerations

How specialized does your solution need to be? Powerful foundation models are constantly improving, making custom model development less necessary in many cases.

- **Latency**: Consider real-time requirements. Do you need instantaneous responses, or can you tolerate some delay? If real-time interaction isn't critical, you have more flexibility in choosing models and infrastructure.
- **Connectivity**:  Consider offline functionality. Are there scenarios where the solution needs to function without internet access?
- **Accuracy**: Define acceptable tolerances. Determine the level of accuracy required for your AI's output. This will influence model selection, training data, and evaluation metrics.
- **Explainability**: Transparency is key. In certain domains, understanding the AI's reasoning is crucial. Consider using models or techniques that offer explainability features, especially in healthcare or finance.


## Making decisions

You understand your needs and resources. Now it’s time to make decisions such as what model you will use. By carefully considering the factors covered below and conducting thorough research, you can make informed decisions about gen AI pricing and choose the most cost-effective solution for your needs.

### Comparing different companies and models

- *Evaluate model capabilities*: Look for benchmarks and comparisons to assess the quality and performance of different models.
- *Compare pricing structures*: Consider your expected usage and calculate the costs based on the pricing models and metrics of each provider.
- *Factor in additional costs*: Account for potential costs like data storage, API calls, and fine-tuning.
- *Read the fine print*: Pay attention to usage limits, data privacy policies, and other terms of service.

### Key resources for comparison

- *Provider websites*: Check the pricing pages of different companies.
- *Research papers and benchmarks*: Look for independent evaluations and comparisons of different models.
- *Community forums and discussions*: Engage with other users and experts to learn about their experiences and pricing insights.

## Maintenance

Building a generative AI solution is a journey, not a destination. To maximize your long-term success, it's essential to consider the ongoing maintenance and evolution of your project from the beginning.
- How will your project be maintained?
- Do you have the resources in place to maintain the project over time?
- What specific maintenance needs will your project have?

By proactively planning for maintenance, you'll ensure the long-term success and sustainability of your generative AI solution. Google Cloud provides a fully managed environment, which allows developers to focus on agent development rather than infrastructure and maintenance. With the platform handling deployment and model improvement, developers can maximize their return on investment and avoid costly issues down the road.

1. **Model monitoring and retraining** - Establish a system to continuously monitor your model's performance and retrain it periodically with updated data.
2. **Software updates and bug fixes** - Stay informed about updates to your chosen AI platform, libraries, or frameworks, and implement them promptly to ensure optimal performance and security.
3. **Data updates** - Plan for regular data updates to keep your model fresh and relevant. This might involve adding new data sources, cleaning existing data, or adapting to evolving data formats.
4. **Hardware and infrastructure** - Consider the maintenance needs of your hardware and infrastructure. This includes server maintenance, security updates, and capacity planning to accommodate growing data and user demands.
5. **Security and compliance** - Maintain a vigilant approach to security. Regularly review and update your security measures to protect against evolving threats. Ensure ongoing compliance with data privacy regulations as they change.